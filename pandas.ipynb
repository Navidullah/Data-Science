{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange;\">Pandas in Python</h1>\n",
    "<p style=\"font-size:18px;\">Pandas is a powerful and popular data manipulation and analysis library in Python. It provides data structures like Series and DataFrame that are well-suited for working with structured data, such as spreadsheets or SQL tables. Pandas makes it easy to clean, analyze, and visualize data, making it a fundamental tool for data scientists, analysts, and engineers. Here's an overview of how to use Pandas in Python:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Inspecting a Dataframe</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n",
    "\n",
    "<li>.head() returns the first few rows (the “head” of the DataFrame).</li>\n",
    "<li>.info() shows information on each of the columns, such as the data type and number of missing values.</li>\n",
    "<li>.shape returns the number of rows and columns of the DataFrame.</li>\n",
    "<li>.describe() calculates a few summary statistics for each column.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>4887681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>735139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>7158024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3009733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region       state  individuals  family_members  state_pop\n",
       "0  East South Central     Alabama       2570.0           864.0    4887681\n",
       "1             Pacific      Alaska       1434.0           582.0     735139\n",
       "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
       "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
       "4             Pacific  California     109008.0         20964.0   39461588"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import the cars.csv data: cars\n",
    "home_data = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/home_data.csv')\n",
    "\n",
    "home_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   region          51 non-null     object \n",
      " 1   state           51 non-null     object \n",
      " 2   individuals     51 non-null     float64\n",
      " 3   family_members  51 non-null     float64\n",
      " 4   state_pop       51 non-null     int64  \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 2.1+ KB\n"
     ]
    }
   ],
   "source": [
    "home_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 5)"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>5.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7225.784314</td>\n",
       "      <td>3504.882353</td>\n",
       "      <td>6.405637e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15991.025083</td>\n",
       "      <td>7805.411811</td>\n",
       "      <td>7.327258e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>434.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>5.776010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1446.500000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>1.777414e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3082.000000</td>\n",
       "      <td>1482.000000</td>\n",
       "      <td>4.461153e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6781.500000</td>\n",
       "      <td>3196.000000</td>\n",
       "      <td>7.340946e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>109008.000000</td>\n",
       "      <td>52070.000000</td>\n",
       "      <td>3.946159e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         individuals  family_members     state_pop\n",
       "count      51.000000       51.000000  5.100000e+01\n",
       "mean     7225.784314     3504.882353  6.405637e+06\n",
       "std     15991.025083     7805.411811  7.327258e+06\n",
       "min       434.000000       75.000000  5.776010e+05\n",
       "25%      1446.500000      592.000000  1.777414e+06\n",
       "50%      3082.000000     1482.000000  4.461153e+06\n",
       "75%      6781.500000     3196.000000  7.340946e+06\n",
       "max    109008.000000    52070.000000  3.946159e+07"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Parts of a DataFrame</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand DataFrame objects, it's useful to know that they consist of three components, stored as attributes:\n",
    "\n",
    "<li>.values: A two-dimensional NumPy array of values.</li>\n",
    "<li>.columns: An index of columns: the column names.</li>\n",
    "<li>.index: An index for the rows: either row numbers or row names.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Print a 2D NumPy array of the values in homelessness.</li>\n",
    "<li>Print the column names of homelessness.</li>\n",
    "<li>Print the index of homelessness.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n",
      " ['Pacific' 'Alaska' 1434.0 582.0 735139]\n",
      " ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n",
      " ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n",
      " ['Pacific' 'California' 109008.0 20964.0 39461588]\n",
      " ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n",
      " ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n",
      " ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n",
      " ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n",
      " ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n",
      " ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n",
      " ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n",
      " ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n",
      " ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n",
      " ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n",
      " ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n",
      " ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n",
      " ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n",
      " ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n",
      " ['New England' 'Maine' 1450.0 1066.0 1339057]\n",
      " ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n",
      " ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n",
      " ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n",
      " ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n",
      " ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n",
      " ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n",
      " ['Mountain' 'Montana' 983.0 422.0 1060665]\n",
      " ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n",
      " ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n",
      " ['New England' 'New Hampshire' 835.0 615.0 1353465]\n",
      " ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n",
      " ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n",
      " ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n",
      " ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n",
      " ['West North Central' 'North Dakota' 467.0 75.0 758080]\n",
      " ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n",
      " ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n",
      " ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n",
      " ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n",
      " ['New England' 'Rhode Island' 747.0 354.0 1058287]\n",
      " ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n",
      " ['West North Central' 'South Dakota' 836.0 323.0 878698]\n",
      " ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n",
      " ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n",
      " ['Mountain' 'Utah' 1904.0 972.0 3153550]\n",
      " ['New England' 'Vermont' 780.0 511.0 624358]\n",
      " ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n",
      " ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n",
      " ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n",
      " ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n",
      " ['Mountain' 'Wyoming' 434.0 205.0 577601]]\n",
      "Index(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\n",
      "RangeIndex(start=0, stop=51, step=1)\n"
     ]
    }
   ],
   "source": [
    "# Print the values of homelessness\n",
    "print(home_data.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(home_data.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(home_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Sorting rows</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to .sort_values().<br>\n",
    "\n",
    "In cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen;\">Sort homelessness by the number of homeless individuals in the individuals column, from smallest to largest, and save this as home_data_ind.\n",
    "Print the head of the sorted DataFrame.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>434.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>577601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>467.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>758080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>708.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>965479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>New England</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>747.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>1058287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>New England</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>780.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>624358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region         state  individuals  family_members  state_pop\n",
       "50            Mountain       Wyoming        434.0           205.0     577601\n",
       "34  West North Central  North Dakota        467.0            75.0     758080\n",
       "7       South Atlantic      Delaware        708.0           374.0     965479\n",
       "39         New England  Rhode Island        747.0           354.0    1058287\n",
       "45         New England       Vermont        780.0           511.0     624358"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort homelessness by individuals\n",
    "home_data_ind = home_data.sort_values('individuals')\n",
    "\n",
    "# Print the top few rows\n",
    "home_data_ind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen;\">Sort homelessness by the number of homeless family_members in descending order, and save this as home_data_fam.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mid-Atlantic</td>\n",
       "      <td>New York</td>\n",
       "      <td>39827.0</td>\n",
       "      <td>52070.0</td>\n",
       "      <td>19530351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New England</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>6811.0</td>\n",
       "      <td>13257.0</td>\n",
       "      <td>6882635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Florida</td>\n",
       "      <td>21443.0</td>\n",
       "      <td>9587.0</td>\n",
       "      <td>21244317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Texas</td>\n",
       "      <td>19199.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>28628666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region          state  individuals  family_members  state_pop\n",
       "32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n",
       "4              Pacific     California     109008.0         20964.0   39461588\n",
       "21         New England  Massachusetts       6811.0         13257.0    6882635\n",
       "9       South Atlantic        Florida      21443.0          9587.0   21244317\n",
       "43  West South Central          Texas      19199.0          6111.0   28628666"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort homelessness by individuals\n",
    "home_data_fam = home_data.sort_values('family_members', ascending=False)\n",
    "\n",
    "# Print the top few rows\n",
    "home_data_fam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen;\">Sort homelessness first by region (ascending), and then by number of family members (descending). Save this as home_data_reg_fam.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>6752.0</td>\n",
       "      <td>3891.0</td>\n",
       "      <td>12723071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>6929.0</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>11676341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>5209.0</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>9984072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>2167.0</td>\n",
       "      <td>5807406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>3776.0</td>\n",
       "      <td>1482.0</td>\n",
       "      <td>6695497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region      state  individuals  family_members  state_pop\n",
       "13  East North Central   Illinois       6752.0          3891.0   12723071\n",
       "35  East North Central       Ohio       6929.0          3320.0   11676341\n",
       "22  East North Central   Michigan       5209.0          3142.0    9984072\n",
       "49  East North Central  Wisconsin       2740.0          2167.0    5807406\n",
       "14  East North Central    Indiana       3776.0          1482.0    6695497"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort homelessness by region, then descending family members\n",
    "home_data_reg_fam = home_data.sort_values(['region','family_members'],ascending=[True,False])\n",
    "\n",
    "# Print the top few rows\n",
    "home_data_reg_fam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Subsetting the Columns</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only specific column of the DataFrame df,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen;\">Create a Series called individuals that contains only the individuals column of home_data.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2570.0\n",
      "1      1434.0\n",
      "2      7259.0\n",
      "3      2280.0\n",
      "4    109008.0\n",
      "Name: individuals, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "individuals = home_data['individuals']\n",
    "print(individuals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Subsetting rows</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as filtering rows or selecting rows.\n",
    "\n",
    "\n",
    "<span style=\"color:lightgreen;\">Filter home_data for cases where the number of individuals is greater than ten thousand, assigning to ind_gt_10k. View the printed result.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                region       state  individuals  family_members  state_pop\n",
      "4              Pacific  California     109008.0         20964.0   39461588\n",
      "9       South Atlantic     Florida      21443.0          9587.0   21244317\n",
      "32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n",
      "37             Pacific      Oregon      11139.0          3337.0    4181886\n",
      "43  West South Central       Texas      19199.0          6111.0   28628666\n",
      "47             Pacific  Washington      16424.0          5880.0    7523869\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = home_data[home_data['individuals']>10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen;\">Filter homelessness for cases where the USA Census region is \"Mountain\", assigning to mountain_reg. View the printed result.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region       state  individuals  family_members  state_pop\n",
      "2   Mountain     Arizona       7259.0          2606.0    7158024\n",
      "5   Mountain    Colorado       7607.0          3250.0    5691287\n",
      "12  Mountain       Idaho       1297.0           715.0    1750536\n",
      "26  Mountain     Montana        983.0           422.0    1060665\n",
      "28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "31  Mountain  New Mexico       1949.0           602.0    2092741\n",
      "44  Mountain        Utah       1904.0           972.0    3153550\n",
      "50  Mountain     Wyoming        434.0           205.0     577601\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = home_data[home_data['region']=='Mountain']\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:lightgreen;\">Filter for rows where family_members is less than 1000 and region is Pacific</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    region   state  individuals  family_members  state_pop\n",
      "1  Pacific  Alaska       1434.0           582.0     735139\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = home_data[(home_data['region']=='Pacific') & (home_data['family_members'] <1000)]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen;\">Subsetting rows by categorical variables</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting data based on a categorical variable often involves using the \"or\" operator (|) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.<br>\n",
    "\n",
    "colors = [\"brown\", \"black\", \"tan\"]<br>\n",
    "condition = dogs[\"color\"].isin(colors)<br>\n",
    "dogs[condition]\n",
    "\n",
    "<span style=\"color:lightgreen;\">Filter home_data for cases where the USA census state is in the list of Mojave states, canu, assigning to mojave_home_data. View the printed result.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region       state  individuals  family_members  state_pop\n",
      "2   Mountain     Arizona       7259.0          2606.0    7158024\n",
      "4    Pacific  California     109008.0         20964.0   39461588\n",
      "28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "44  Mountain        Utah       1904.0           972.0    3153550\n"
     ]
    }
   ],
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_home_data = home_data[home_data['state'].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_home_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Adding new Columns</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You aren't stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.<br>\n",
    "\n",
    "You can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units.<br>\n",
    "\n",
    "home_data is a DataFrame containing estimates of home_data in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state's total population.<br>\n",
    "\n",
    "home_data is available and pandas is loaded as pd.<br>\n",
    "\n",
    "<li style='color:lightGreen;'>Add a new column to home_data, named total, containing the sum of the individuals and family_members columns.</li>\n",
    "<li style='color:lightGreen;'>Add another column to home_data, named p_homeless, containing the proportion of the total homeless population to the total population in each state state_pop.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>individuals</th>\n",
       "      <th>family_members</th>\n",
       "      <th>state_pop</th>\n",
       "      <th>total</th>\n",
       "      <th>p_homeless</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>4887681</td>\n",
       "      <td>3434.0</td>\n",
       "      <td>0.000703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>735139</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0.002742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>2606.0</td>\n",
       "      <td>7158024</td>\n",
       "      <td>9865.0</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3009733</td>\n",
       "      <td>2712.0</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>20964.0</td>\n",
       "      <td>39461588</td>\n",
       "      <td>129972.0</td>\n",
       "      <td>0.003294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>7607.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>5691287</td>\n",
       "      <td>10857.0</td>\n",
       "      <td>0.001908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New England</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>3571520</td>\n",
       "      <td>3976.0</td>\n",
       "      <td>0.001113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>708.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>965479</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>0.001121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>3134.0</td>\n",
       "      <td>701547</td>\n",
       "      <td>6904.0</td>\n",
       "      <td>0.009841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Florida</td>\n",
       "      <td>21443.0</td>\n",
       "      <td>9587.0</td>\n",
       "      <td>21244317</td>\n",
       "      <td>31030.0</td>\n",
       "      <td>0.001461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>2556.0</td>\n",
       "      <td>10511131</td>\n",
       "      <td>9499.0</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>4131.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>1420593</td>\n",
       "      <td>6530.0</td>\n",
       "      <td>0.004597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>1750536</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>6752.0</td>\n",
       "      <td>3891.0</td>\n",
       "      <td>12723071</td>\n",
       "      <td>10643.0</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>3776.0</td>\n",
       "      <td>1482.0</td>\n",
       "      <td>6695497</td>\n",
       "      <td>5258.0</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>3148618</td>\n",
       "      <td>2749.0</td>\n",
       "      <td>0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>2911359</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>4461153</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>0.000827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>4659690</td>\n",
       "      <td>3059.0</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>New England</td>\n",
       "      <td>Maine</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>1339057</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.001879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>4914.0</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>6035802</td>\n",
       "      <td>7144.0</td>\n",
       "      <td>0.001184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New England</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>6811.0</td>\n",
       "      <td>13257.0</td>\n",
       "      <td>6882635</td>\n",
       "      <td>20068.0</td>\n",
       "      <td>0.002916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>5209.0</td>\n",
       "      <td>3142.0</td>\n",
       "      <td>9984072</td>\n",
       "      <td>8351.0</td>\n",
       "      <td>0.000836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>3993.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>5606249</td>\n",
       "      <td>7243.0</td>\n",
       "      <td>0.001292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>2981020</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>3776.0</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>6121623</td>\n",
       "      <td>5883.0</td>\n",
       "      <td>0.000961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Montana</td>\n",
       "      <td>983.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>1060665</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>1745.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>1925614</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>0.001257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>7058.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>3027341</td>\n",
       "      <td>7544.0</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New England</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>835.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>1353465</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>0.001071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mid-Atlantic</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>6048.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>8886025</td>\n",
       "      <td>9398.0</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>2092741</td>\n",
       "      <td>2551.0</td>\n",
       "      <td>0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mid-Atlantic</td>\n",
       "      <td>New York</td>\n",
       "      <td>39827.0</td>\n",
       "      <td>52070.0</td>\n",
       "      <td>19530351</td>\n",
       "      <td>91897.0</td>\n",
       "      <td>0.004705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>6451.0</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>10381615</td>\n",
       "      <td>9268.0</td>\n",
       "      <td>0.000893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>467.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>758080</td>\n",
       "      <td>542.0</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>6929.0</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>11676341</td>\n",
       "      <td>10249.0</td>\n",
       "      <td>0.000878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>3940235</td>\n",
       "      <td>3871.0</td>\n",
       "      <td>0.000982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>11139.0</td>\n",
       "      <td>3337.0</td>\n",
       "      <td>4181886</td>\n",
       "      <td>14476.0</td>\n",
       "      <td>0.003462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Mid-Atlantic</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>8163.0</td>\n",
       "      <td>5349.0</td>\n",
       "      <td>12800922</td>\n",
       "      <td>13512.0</td>\n",
       "      <td>0.001056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>New England</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>747.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>1058287</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>0.001040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>3082.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>5084156</td>\n",
       "      <td>3933.0</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>836.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>878698</td>\n",
       "      <td>1159.0</td>\n",
       "      <td>0.001319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>6139.0</td>\n",
       "      <td>1744.0</td>\n",
       "      <td>6771631</td>\n",
       "      <td>7883.0</td>\n",
       "      <td>0.001164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Texas</td>\n",
       "      <td>19199.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>28628666</td>\n",
       "      <td>25310.0</td>\n",
       "      <td>0.000884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Utah</td>\n",
       "      <td>1904.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>3153550</td>\n",
       "      <td>2876.0</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>New England</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>780.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>624358</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>0.002068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>3928.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>8501286</td>\n",
       "      <td>5975.0</td>\n",
       "      <td>0.000703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Washington</td>\n",
       "      <td>16424.0</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>7523869</td>\n",
       "      <td>22304.0</td>\n",
       "      <td>0.002964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1804291</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>2167.0</td>\n",
       "      <td>5807406</td>\n",
       "      <td>4907.0</td>\n",
       "      <td>0.000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>434.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>577601</td>\n",
       "      <td>639.0</td>\n",
       "      <td>0.001106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region                 state  individuals  family_members  \\\n",
       "0   East South Central               Alabama       2570.0           864.0   \n",
       "1              Pacific                Alaska       1434.0           582.0   \n",
       "2             Mountain               Arizona       7259.0          2606.0   \n",
       "3   West South Central              Arkansas       2280.0           432.0   \n",
       "4              Pacific            California     109008.0         20964.0   \n",
       "5             Mountain              Colorado       7607.0          3250.0   \n",
       "6          New England           Connecticut       2280.0          1696.0   \n",
       "7       South Atlantic              Delaware        708.0           374.0   \n",
       "8       South Atlantic  District of Columbia       3770.0          3134.0   \n",
       "9       South Atlantic               Florida      21443.0          9587.0   \n",
       "10      South Atlantic               Georgia       6943.0          2556.0   \n",
       "11             Pacific                Hawaii       4131.0          2399.0   \n",
       "12            Mountain                 Idaho       1297.0           715.0   \n",
       "13  East North Central              Illinois       6752.0          3891.0   \n",
       "14  East North Central               Indiana       3776.0          1482.0   \n",
       "15  West North Central                  Iowa       1711.0          1038.0   \n",
       "16  West North Central                Kansas       1443.0           773.0   \n",
       "17  East South Central              Kentucky       2735.0           953.0   \n",
       "18  West South Central             Louisiana       2540.0           519.0   \n",
       "19         New England                 Maine       1450.0          1066.0   \n",
       "20      South Atlantic              Maryland       4914.0          2230.0   \n",
       "21         New England         Massachusetts       6811.0         13257.0   \n",
       "22  East North Central              Michigan       5209.0          3142.0   \n",
       "23  West North Central             Minnesota       3993.0          3250.0   \n",
       "24  East South Central           Mississippi       1024.0           328.0   \n",
       "25  West North Central              Missouri       3776.0          2107.0   \n",
       "26            Mountain               Montana        983.0           422.0   \n",
       "27  West North Central              Nebraska       1745.0           676.0   \n",
       "28            Mountain                Nevada       7058.0           486.0   \n",
       "29         New England         New Hampshire        835.0           615.0   \n",
       "30        Mid-Atlantic            New Jersey       6048.0          3350.0   \n",
       "31            Mountain            New Mexico       1949.0           602.0   \n",
       "32        Mid-Atlantic              New York      39827.0         52070.0   \n",
       "33      South Atlantic        North Carolina       6451.0          2817.0   \n",
       "34  West North Central          North Dakota        467.0            75.0   \n",
       "35  East North Central                  Ohio       6929.0          3320.0   \n",
       "36  West South Central              Oklahoma       2823.0          1048.0   \n",
       "37             Pacific                Oregon      11139.0          3337.0   \n",
       "38        Mid-Atlantic          Pennsylvania       8163.0          5349.0   \n",
       "39         New England          Rhode Island        747.0           354.0   \n",
       "40      South Atlantic        South Carolina       3082.0           851.0   \n",
       "41  West North Central          South Dakota        836.0           323.0   \n",
       "42  East South Central             Tennessee       6139.0          1744.0   \n",
       "43  West South Central                 Texas      19199.0          6111.0   \n",
       "44            Mountain                  Utah       1904.0           972.0   \n",
       "45         New England               Vermont        780.0           511.0   \n",
       "46      South Atlantic              Virginia       3928.0          2047.0   \n",
       "47             Pacific            Washington      16424.0          5880.0   \n",
       "48      South Atlantic         West Virginia       1021.0           222.0   \n",
       "49  East North Central             Wisconsin       2740.0          2167.0   \n",
       "50            Mountain               Wyoming        434.0           205.0   \n",
       "\n",
       "    state_pop     total  p_homeless  \n",
       "0     4887681    3434.0    0.000703  \n",
       "1      735139    2016.0    0.002742  \n",
       "2     7158024    9865.0    0.001378  \n",
       "3     3009733    2712.0    0.000901  \n",
       "4    39461588  129972.0    0.003294  \n",
       "5     5691287   10857.0    0.001908  \n",
       "6     3571520    3976.0    0.001113  \n",
       "7      965479    1082.0    0.001121  \n",
       "8      701547    6904.0    0.009841  \n",
       "9    21244317   31030.0    0.001461  \n",
       "10   10511131    9499.0    0.000904  \n",
       "11    1420593    6530.0    0.004597  \n",
       "12    1750536    2012.0    0.001149  \n",
       "13   12723071   10643.0    0.000837  \n",
       "14    6695497    5258.0    0.000785  \n",
       "15    3148618    2749.0    0.000873  \n",
       "16    2911359    2216.0    0.000761  \n",
       "17    4461153    3688.0    0.000827  \n",
       "18    4659690    3059.0    0.000656  \n",
       "19    1339057    2516.0    0.001879  \n",
       "20    6035802    7144.0    0.001184  \n",
       "21    6882635   20068.0    0.002916  \n",
       "22    9984072    8351.0    0.000836  \n",
       "23    5606249    7243.0    0.001292  \n",
       "24    2981020    1352.0    0.000454  \n",
       "25    6121623    5883.0    0.000961  \n",
       "26    1060665    1405.0    0.001325  \n",
       "27    1925614    2421.0    0.001257  \n",
       "28    3027341    7544.0    0.002492  \n",
       "29    1353465    1450.0    0.001071  \n",
       "30    8886025    9398.0    0.001058  \n",
       "31    2092741    2551.0    0.001219  \n",
       "32   19530351   91897.0    0.004705  \n",
       "33   10381615    9268.0    0.000893  \n",
       "34     758080     542.0    0.000715  \n",
       "35   11676341   10249.0    0.000878  \n",
       "36    3940235    3871.0    0.000982  \n",
       "37    4181886   14476.0    0.003462  \n",
       "38   12800922   13512.0    0.001056  \n",
       "39    1058287    1101.0    0.001040  \n",
       "40    5084156    3933.0    0.000774  \n",
       "41     878698    1159.0    0.001319  \n",
       "42    6771631    7883.0    0.001164  \n",
       "43   28628666   25310.0    0.000884  \n",
       "44    3153550    2876.0    0.000912  \n",
       "45     624358    1291.0    0.002068  \n",
       "46    8501286    5975.0    0.000703  \n",
       "47    7523869   22304.0    0.002964  \n",
       "48    1804291    1243.0    0.000689  \n",
       "49    5807406    4907.0    0.000845  \n",
       "50     577601     639.0    0.001106  "
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add total col as sum of individuals and family_members\n",
    "home_data[\"total\"]=home_data['individuals']+home_data['family_members']\n",
    "\n",
    "# Add p_homeless col as proportion of total homeless population to the state population\n",
    "home_data['p_homeless']=home_data['total'] / home_data['state_pop']\n",
    "\n",
    "# See the result\n",
    "home_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.<br>\n",
    "\n",
    "In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\" Combine your new pandas skills to find out.<br>\n",
    "\n",
    "<li style='color:lightGreen;'>Add a column to homelessness, indiv_per_10k, containing the number of homeless individuals per ten thousand people in each state, using state_pop for state population.</li>\n",
    "<li style='color:lightGreen;'>Subset rows where indiv_per_10k is higher than 20, assigning to high_homelessness.</li>\n",
    "<li style='color:lightGreen;'>Sort high_homelessness by descending indiv_per_10k, assigning to high_homelessness_srt.</li>\n",
    "<li style='color:lightGreen;'>Select only the state and indiv_per_10k columns of high_homelessness_srt and save as result. Look at the result.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   state  indiv_per_10k\n",
      "8   District of Columbia      53.738381\n",
      "11                Hawaii      29.079406\n",
      "4             California      27.623825\n",
      "37                Oregon      26.636307\n",
      "28                Nevada      23.314189\n",
      "47            Washington      21.829195\n",
      "32              New York      20.392363\n"
     ]
    }
   ],
   "source": [
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "home_data[\"indiv_per_10k\"] = 10000 * home_data['individuals'] /home_data['state_pop'] \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = home_data[home_data['indiv_per_10k'] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt =high_homelessness.sort_values('indiv_per_10k',ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "#result = high_homelessness_srt.loc[:,['state','indiv_per_10k']]\n",
    "result = high_homelessness_srt[['state','indiv_per_10k']]\n",
    "\n",
    "# See the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool combination! District of Columbia has the highest number of homeless individuals - almost 54 per ten thousand people. This is almost double the number of the next-highest state, Hawaii. If you combine new column addition, row subsetting, sorting, and column selection, you can answer lots of questions like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Dictionary to DataFrame</h3>\n",
    "<p style=\"font-size:18px;\">Pandas is an open source library, providing high-performance, easy-to-use data structures and data analysis tools for Python. Sounds promising!\n",
    "\n",
    "The DataFrame is one of Pandas' most important data structures. It's basically a way to store tabular data where you can label the rows and the columns. One way to build a DataFrame is from a dictionary.\n",
    "\n",
    "In the exercises that follow you will be working with vehicle data from different countries. Each observation corresponds to a country and the columns give information about the number of vehicles per capita, whether people drive left or right, and so on.</p>\n",
    "\n",
    "<li>names, containing the country names for which data is available.</li>\n",
    "<li>dr, a list with booleans that tells whether people drive left or right in the corresponding country.</li>\n",
    "<li>cpc, the number of motor vehicles per 1000 people in the corresponding country.</li>\n",
    "\n",
    "<p>Each dictionary key is a column label and each value is a list which contains the column elements.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Import pandas as pd.</li>\n",
    "<li>Use the pre-defined lists to create a dictionary called my_dict. There should be three key value pairs:</li>\n",
    "<li>key 'country' and value names.</li>\n",
    "<li>key 'drives_right' and value dr.</li>\n",
    "<li>key 'cars_per_cap' and value cpc.</li>\n",
    "<li>Use pd.DataFrame() to turn your dict into a DataFrame called cars.</li>\n",
    "<li>Print out cars and see how beautiful it is.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         country  drives_right  cars_per_cap\n",
      "0  United States          True           809\n",
      "1      Australia         False           731\n",
      "2          Japan         False           588\n",
      "3          India         False            18\n",
      "4         Russia          True           200\n",
      "5        Morocco          True            70\n",
      "6          Egypt          True            45\n"
     ]
    }
   ],
   "source": [
    "# Pre-defined lists\n",
    "names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\n",
    "dr =  [True, False, False, False, True, True, True]\n",
    "cpc = [809, 731, 588, 18, 200, 70, 45]\n",
    "\n",
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create dictionary my_dict with three key:value pairs: my_dict\n",
    "my_dict ={\n",
    "    'country': names,\n",
    "    'drives_right':dr,\n",
    "    'cars_per_cap':cpc\n",
    "}\n",
    "\n",
    "\n",
    "# Build a DataFrame cars from my_dict: cars\n",
    "cars = pd.DataFrame(my_dict)\n",
    "\n",
    "\n",
    "# Print cars\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Python code that solves the previous exercise is included in the script. Have you noticed that the row labels (i.e. the labels for the different observations) were automatically set to integers from 0 up to 6?\n",
    "\n",
    "To solve this a list row_labels has been created. You can use it to specify the row labels of the cars DataFrame. You do this by setting the index attribute of cars, that you can access as cars.index.</p>\n",
    "\n",
    "<li>Specify the row labels by setting cars.index equal to row_labels.</li>\n",
    "<li>Print out cars again and check if the row labels are correct this time.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country  drives_right  cars_per_cap\n",
      "US   United States          True           809\n",
      "AUS      Australia         False           731\n",
      "JPN          Japan         False           588\n",
      "IN           India         False            18\n",
      "RU          Russia          True           200\n",
      "MOR        Morocco          True            70\n",
      "EG           Egypt          True            45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build cars DataFrame\n",
    "names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\n",
    "dr =  [True, False, False, False, True, True, True]\n",
    "cpc = [809, 731, 588, 18, 200, 70, 45]\n",
    "cars_dict = { 'country':names, 'drives_right':dr, 'cars_per_cap':cpc }\n",
    "cars = pd.DataFrame(cars_dict)\n",
    "\n",
    "\n",
    "# Definition of row_labels\n",
    "row_labels = ['US', 'AUS', 'JPN', 'IN', 'RU', 'MOR', 'EG']\n",
    "\n",
    "# Specify row labels of cars\n",
    "cars.index =row_labels\n",
    "\n",
    "\n",
    "# Print cars again\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">CSV to DataFrame</h3>\n",
    "<p>Putting data in a dictionary and then building a DataFrame works, but it's not very efficient. What if you're dealing with millions of observations? In those cases, the data is typically available as files with a regular structure. One of those file types is the CSV file, which is short for \"comma-separated values\".\n",
    "\n",
    "To import CSV data into Python as a Pandas DataFrame you can use read_csv().\n",
    "\n",
    "Let's explore this function with the same cars data from the previous exercises. This time, however, the data is available in a CSV file, named cars.csv. It is available in your current working directory, so the path to the file is simply 'cars.csv'.\n",
    "<li>To import CSV files you still need the pandas package: import it as pd.</li>\n",
    "<li>Use pd.read_csv() to import cars.csv data as a DataFrame. Store this DataFrame as cars.</li>\n",
    "<li>Print out cars. Does everything look OK?</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0        country drives_right  cars_per_cap\n",
      "0         US  United States         True           809\n",
      "1        AUS      Australia        False           731\n",
      "2        JPN          Japan        False           588\n",
      "3         IN          India        False            18\n",
      "4         RU         Russia         True           200\n",
      "5        MOR        Morocco         True            70\n",
      "6         EG          Egypt         True            45\n"
     ]
    }
   ],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import the cars.csv data: cars\n",
    "cars = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/cars.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print out cars\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Your read_csv() call to import the CSV data didn't generate an error, but the output is not entirely what we wanted. The row labels were imported as another column without a name.\n",
    "\n",
    "Remember index_col, an argument of read_csv(), that you can use to specify which column in the CSV file should be used as a row label? Well, that's exactly what you need here!\n",
    "\n",
    "Python code that solves the previous exercise is already included; can you make the appropriate changes to fix the data import?\n",
    "<li>Run the code with Run Code and assert that the first column should actually be used as row labels.\n",
    "\n",
    "</li>\n",
    "<li>Specify the index_col argument inside pd.read_csv(): set it to 0, so that the first column is used as row labels.</li>\n",
    "<li>Has the printout of cars improved now?</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country drives_right  cars_per_cap\n",
      "US   United States         True           809\n",
      "AUS      Australia        False           731\n",
      "JPN          Japan        False           588\n",
      "IN           India        False            18\n",
      "RU          Russia         True           200\n",
      "MOR        Morocco         True            70\n",
      "EG           Egypt         True            45\n"
     ]
    }
   ],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Fix import by including index_col\n",
    "cars = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/cars.txt',index_col = 0)\n",
    "\n",
    "# Print out cars\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Accessing DataFrames</h3>\n",
    "<h3 style=\"color:orange;\">Square Brackets Technique</h3>\n",
    "<p style=\"font-size:18px;\">In the sample code, the same cars data is imported from a CSV files as a Pandas DataFrame.<br> To select only the cars_per_cap column from cars, you can use:\n",
    "\n",
    "cars['cars_per_cap'] <br>\n",
    "cars[['cars_per_cap']] <br>\n",
    "The single bracket version gives a Pandas Series, the double bracket version gives a Pandas DataFrame.</p>\n",
    "\n",
    "<li>Use single square brackets to print out the country column of cars as a Pandas Series.</li>\n",
    "<li>Use double square brackets to print out the country column of cars as a Pandas DataFrame.</li>\n",
    "<li>Use double square brackets to print out a DataFrame with both the country and drives_right columns of cars, in this order.</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US     United States\n",
      "AUS        Australia\n",
      "JPN            Japan\n",
      "IN             India\n",
      "RU            Russia\n",
      "MOR          Morocco\n",
      "EG             Egypt\n",
      "Name: country, dtype: object\n",
      "           country\n",
      "US   United States\n",
      "AUS      Australia\n",
      "JPN          Japan\n",
      "IN           India\n",
      "RU          Russia\n",
      "MOR        Morocco\n",
      "EG           Egypt\n",
      "           country drives_right\n",
      "US   United States         True\n",
      "AUS      Australia        False\n",
      "JPN          Japan        False\n",
      "IN           India        False\n",
      "RU          Russia         True\n",
      "MOR        Morocco         True\n",
      "EG           Egypt         True\n"
     ]
    }
   ],
   "source": [
    "# Import cars data\n",
    "import pandas as pd\n",
    "cars = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/cars.txt', index_col = 0)\n",
    "\n",
    "# Print out country column as Pandas Series\n",
    "print(cars['country'])\n",
    "\n",
    "\n",
    "\n",
    "# Print out country column as Pandas DataFrame\n",
    "print(cars[['country']])\n",
    "\n",
    "\n",
    "# Print out DataFrame with country and drives_right columns\n",
    "print(cars[['country','drives_right']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:18px;'><span style=\"color:orange;\">Square brackets</span> can do more than just selecting columns. You can also use them to get rows, or observations, from a DataFrame. The following call selects the first five rows from the cars DataFrame:\n",
    "\n",
    "cars[0:5]\n",
    "The result is another DataFrame containing only the rows you specified.\n",
    "\n",
    "<span style=\"color:red\">Pay attention:</span> You can only select rows using square brackets if you specify a slice, like 0:4. Also, you're using the integer indexes of the rows here, not the row labels!</p>\n",
    "\n",
    "<li>Select the first 3 observations from cars and print them out.</li>\n",
    "<li>Select the fourth, fifth and sixth observation, corresponding to row indexes 3, 4 and 5, and print them out.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country drives_right  cars_per_cap\n",
      "US   United States         True           809\n",
      "AUS      Australia        False           731\n",
      "JPN          Japan        False           588\n",
      "     country drives_right  cars_per_cap\n",
      "IN     India        False            18\n",
      "RU    Russia         True           200\n",
      "MOR  Morocco         True            70\n"
     ]
    }
   ],
   "source": [
    "# Import cars data\n",
    "import pandas as pd\n",
    "cars = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/cars.txt', index_col = 0)\n",
    "\n",
    "# Print out first 3 observations\n",
    "print(cars[0:3])\n",
    "\n",
    "\n",
    "# Print out fourth, fifth and sixth observation\n",
    "print(cars[3:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:18px;'><span style=\"color:orange;\">loc and iloc</span> With loc and iloc you can do practically any data selection operation on DataFrames you can think of. loc is label-based, which means that you have to specify rows and columns based on their row and column labels. iloc is integer index based, so you have to specify rows and columns by their integer index like you did in the previous exercise.\n",
    "\n",
    "Try out the following commands in the IPython Shell to experiment with loc and iloc to select observations. Each pair of commands here gives the same result.\n",
    "\n",
    "cars.loc['RU'] <br>\n",
    "cars.iloc[4]<br>\n",
    "\n",
    "cars.loc[['RU']]<br>\n",
    "cars.iloc[[4]]<br>\n",
    "\n",
    "cars.loc[['RU', 'AUS']]<br>\n",
    "cars.iloc[[4, 1]]<br>\n",
    "As before, code is included that imports the cars data as a Pandas DataFrame.</p>\n",
    "\n",
    "<li>Use loc or iloc to select the observation corresponding to Japan as a Series. The label of this row is JPN, the index is 2. Make sure to print the resulting Series.\n",
    "</li>\n",
    "<li>Use loc or iloc to select the observations for Australia and Egypt as a DataFrame.</li> <br>You can find out about the labels/indexes of these rows by inspecting cars in the IPython Shell. Make sure to print the resulting DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country         Japan\n",
      "drives_right    False\n",
      "cars_per_cap      588\n",
      "Name: JPN, dtype: object\n",
      "country         Japan\n",
      "drives_right    False\n",
      "cars_per_cap      588\n",
      "Name: JPN, dtype: object\n",
      "       country drives_right  cars_per_cap\n",
      "AUS  Australia        False           731\n",
      "EG       Egypt         True            45\n",
      "       country drives_right  cars_per_cap\n",
      "AUS  Australia        False           731\n",
      "EG       Egypt         True            45\n"
     ]
    }
   ],
   "source": [
    "# Print out observation for Japan\n",
    "print(cars.loc['JPN'])\n",
    "print(cars.iloc[2])\n",
    "\n",
    "# Print out observations for Australia and Egypt\n",
    "print(cars.loc[['AUS','EG']])\n",
    "print(cars.iloc[[1,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color:orange;\">loc and iloc</span> also allow you to select both rows and columns from a DataFrame. To experiment, try out the following commands in the IPython Shell. Again, paired commands produce the same result.\n",
    "\n",
    "cars.loc['IN', 'cars_per_cap']<br>\n",
    "cars.iloc[3, 0]<br>\n",
    "\n",
    "cars.loc[['IN', 'RU'], 'cars_per_cap']<br>\n",
    "cars.iloc[[3, 4], 0]<br>\n",
    "\n",
    "cars.loc[['IN', 'RU'], ['cars_per_cap', 'country']]<br>\n",
    "cars.iloc[[3, 4], [0, 1]]<br>\n",
    "</p>\n",
    "<li>Print out the drives_right value of the row corresponding to Morocco (its row label is MOR)\n",
    "</li>\n",
    "<li>Print out a sub-DataFrame, containing the observations for Russia and Morocco and the columns country and drives_right.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    drives_right\n",
      "MOR         True\n",
      "\n",
      "     country drives_right\n",
      "RU    Russia         True\n",
      "MOR  Morocco         True\n"
     ]
    }
   ],
   "source": [
    "# Print out drives_right value of Morocco\n",
    "print(cars.loc[[\"MOR\"],['drives_right']])\n",
    "print()\n",
    "\n",
    "\n",
    "# Print sub-DataFrame\n",
    "print(cars.loc[[\"RU\",\"MOR\"],[\"country\",\"drives_right\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It's also possible to select only columns with <span style=\"color:orange;\">loc and iloc</span>. In both cases, you simply put a slice going from beginning to end in front of the comma:\n",
    "\n",
    "cars.loc[:, 'country']<br>\n",
    "cars.iloc[:, 1]<br>\n",
    "\n",
    "cars.loc[:, ['country','drives_right']]<br>\n",
    "cars.iloc[:, [1, 2]]<br>\n",
    "</p>\n",
    "<li>Print out the drives_right column as a Series using loc or iloc.<br></li>\n",
    "<li>Print out the drives_right column as a DataFrame using loc or iloc.<br></li>\n",
    "<li>Print out both the cars_per_cap and drives_right column as a DataFrame using loc or iloc.<br></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US      True\n",
      "AUS    False\n",
      "JPN    False\n",
      "IN     False\n",
      "RU      True\n",
      "MOR     True\n",
      "EG      True\n",
      "Name: drives_right, dtype: object\n",
      "\n",
      "    drives_right\n",
      "US          True\n",
      "AUS        False\n",
      "JPN        False\n",
      "IN         False\n",
      "RU          True\n",
      "MOR         True\n",
      "EG          True\n",
      "\n",
      "     cars_per_cap drives_right\n",
      "US            809         True\n",
      "AUS           731        False\n",
      "JPN           588        False\n",
      "IN             18        False\n",
      "RU            200         True\n",
      "MOR            70         True\n",
      "EG             45         True\n"
     ]
    }
   ],
   "source": [
    "# Print out drives_right column as Series\n",
    "print(cars.loc[:,'drives_right'])\n",
    "print()\n",
    "\n",
    "# Print out drives_right column as DataFrame\n",
    "print(cars.loc[:,['drives_right']])\n",
    "print()\n",
    "\n",
    "# Print out cars_per_cap and drives_right as DataFrame\n",
    "print(cars.loc[:,['cars_per_cap','drives_right']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Summary Statistics</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Mean and Median</h3>\n",
    "Summary statistics are exactly what they sound like - they summarize many numbers in one statistic. For example, mean, median, minimum, maximum, and standard deviation are summary statistics. Calculating summary statistics allows you to get a better sense of your data, even if there's a lot of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sales = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/sales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Explore your new DataFrame first by printing the first few rows of the sales DataFrame.</li>\n",
    "<li>Print information about the columns in sales.</li>\n",
    "<li>Print the mean of the weekly_sales column.</li>\n",
    "<li>Print the median of the weekly_sales column.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store type  department      date  weekly_sales  is_holiday  temperature_c  \\\n",
      "0      1    A           1  2/5/2010      24924.50       False       5.727778   \n",
      "1      1    A           1  3/5/2010      21827.90       False       8.055556   \n",
      "2      1    A           1  4/2/2010      57258.43       False      16.816667   \n",
      "3      1    A           1  5/7/2010      17413.94       False      22.527778   \n",
      "4      1    A           1  6/4/2010      17558.09       False      27.050000   \n",
      "\n",
      "   fuel_price_usd_per_l  unemployment  \n",
      "0              0.679451         8.106  \n",
      "1              0.693452         8.106  \n",
      "2              0.718284         7.808  \n",
      "3              0.748928         7.808  \n",
      "4              0.714586         7.808  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10774 entries, 0 to 10773\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   store                 10774 non-null  int64  \n",
      " 1   type                  10774 non-null  object \n",
      " 2   department            10774 non-null  int64  \n",
      " 3   date                  10774 non-null  object \n",
      " 4   weekly_sales          10774 non-null  float64\n",
      " 5   is_holiday            10774 non-null  bool   \n",
      " 6   temperature_c         10774 non-null  float64\n",
      " 7   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 8   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(2), object(2)\n",
      "memory usage: 684.0+ KB\n",
      "None\n",
      "Mean of weekly_sales is  23843.95014850566\n",
      "Median of weekly_sales is  12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(\"Mean of weekly_sales is \",sales['weekly_sales'].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(\"Median of weekly_sales is \",sales['weekly_sales'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Summarizing Dates</h3>\n",
    "Summary statistics can also be calculated on date columns that have values with the data type datetime64. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers.\n",
    "\n",
    "<li>Print the maximum of the date column.</li>\n",
    "<li>Print the minimum of the date column.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9/2011\n",
      "1/13/2012\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum of the date column\n",
    "print(sales['date'].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales['date'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Efficient summaries</h3>\n",
    "While pandas and NumPy have tons of functions, sometimes, you may need a different function to summarize your data.\n",
    "\n",
    "The .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. For example,\n",
    "\n",
    "df['column'].agg(function)\n",
    "\n",
    "<li>Use the custom iqr function defined for you along with .agg() to print the IQR of the temperature_c column of sales.</li>\n",
    "<li>Update the column selection to use the custom iqr function with .agg() to print the IQR of temperature_c, fuel_price_usd_per_l, and unemployment, in that order.</li>\n",
    "\n",
    "<li>Update the aggregation functions called by .agg(): include iqr and np.median in that order.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.583333337000003\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales['temperature_c'].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_c           16.583333\n",
      "fuel_price_usd_per_l     0.073176\n",
      "unemployment             0.565000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\1446678071.py:7: FutureWarning: The provided callable <function median at 0x00000229FFD83100> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\1446678071.py:7: FutureWarning: The provided callable <function median at 0x00000229FFD83100> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\1446678071.py:7: FutureWarning: The provided callable <function median at 0x00000229FFD83100> is currently using Series.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>fuel_price_usd_per_l</th>\n",
       "      <th>unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iqr</th>\n",
       "      <td>16.583333</td>\n",
       "      <td>0.073176</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>16.966667</td>\n",
       "      <td>0.743381</td>\n",
       "      <td>8.099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temperature_c  fuel_price_usd_per_l  unemployment\n",
       "iqr         16.583333              0.073176         0.565\n",
       "median      16.966667              0.743381         8.099"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Cumulative statistics</h3>\n",
    "Cumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you'll calculate the cumulative sum and cumulative max of a department's weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far.\n",
    "\n",
    "A DataFrame called sales_1_1 has been created for you, which contains the sales data for department 1 of store 1. pandas is loaded as pd.\n",
    "\n",
    "<li>Sort the rows of sales_1_1 by the date column in ascending order.</li>\n",
    "<li>Get the cumulative sum of weekly_sales and add it as a new column of sales_1_1 called cum_weekly_sales.</li>\n",
    "<li>Get the cumulative maximum of weekly_sales, and add it as a column called cum_max_sales.</li>\n",
    "<li>Print the date, weekly_sales, cum_weekly_sales, and cum_max_sales columns.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\1808120347.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sales_1_1['date'] = pd.to_datetime(sales_1_1['date'])  # Convert to datetime\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\1808120347.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sales_1_1['date'] = sales_1_1['date'].dt.strftime('%Y-%m-%d')  # Change format to yyyy-mm-dd\n"
     ]
    }
   ],
   "source": [
    "sales_1_1 = sales[(sales['department']==1) & (sales['store']==1)]\n",
    "\n",
    "# Assuming sales_1_1 is your DataFrame\n",
    "sales_1_1['date'] = pd.to_datetime(sales_1_1['date'])  # Convert to datetime\n",
    "sales_1_1['date'] = sales_1_1['date'].dt.strftime('%Y-%m-%d')  # Change format to yyyy-mm-dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  weekly_sales  cum_weekly_sales  cum_max_sales\n",
      "0   2010-02-05      24924.50          24924.50       24924.50\n",
      "1   2010-03-05      21827.90          46752.40       24924.50\n",
      "2   2010-04-02      57258.43         104010.83       57258.43\n",
      "3   2010-05-07      17413.94         121424.77       57258.43\n",
      "4   2010-06-04      17558.09         138982.86       57258.43\n",
      "5   2010-07-02      16333.14         155316.00       57258.43\n",
      "6   2010-08-06      17508.41         172824.41       57258.43\n",
      "7   2010-09-03      16241.78         189066.19       57258.43\n",
      "8   2010-10-01      20094.19         209160.38       57258.43\n",
      "9   2010-11-05      34238.88         243399.26       57258.43\n",
      "10  2010-12-03      22517.56         265916.82       57258.43\n",
      "11  2011-01-07      15984.24         281901.06       57258.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values('date',ascending=True)\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Dropping duplicates</h3>\n",
    "Removing duplicates is an essential skill to get accurate counts because often, you don't want to count the same thing multiple times. In this exercise, you'll create some new DataFrames using unique values from sales.\n",
    "\n",
    "<li>Remove rows of sales with duplicate pairs of store and type and save as store_types and print the head.</li>\n",
    "<li>Remove rows of sales with duplicate pairs of store and department and save as store_depts and print the head.</li>\n",
    "<li>Subset the rows that are holiday weeks using the is_holiday column, and drop the duplicate dates, saving as holiday_dates.</li>\n",
    "<li>Select the date column of holiday_dates, and print.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_new=sales.iloc[0:50,:]\n",
    "#sales_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      store type  department      date  weekly_sales  is_holiday  \\\n",
      "0         1    A           1  2/5/2010      24924.50       False   \n",
      "901       2    A           1  2/5/2010      35034.06       False   \n",
      "1798      4    A           1  2/5/2010      38724.42       False   \n",
      "2699      6    A           1  2/5/2010      25619.00       False   \n",
      "3593     10    B           1  2/5/2010      40212.84       False   \n",
      "\n",
      "      temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0          5.727778              0.679451         8.106  \n",
      "901        4.550000              0.679451         8.324  \n",
      "1798       6.533333              0.686319         8.623  \n",
      "2699       4.683333              0.679451         7.259  \n",
      "3593      12.411111              0.782478         9.765  \n",
      "    store type  department      date  weekly_sales  is_holiday  temperature_c  \\\n",
      "0       1    A           1  2/5/2010      24924.50       False       5.727778   \n",
      "12      1    A           2  2/5/2010      50605.27       False       5.727778   \n",
      "24      1    A           3  2/5/2010      13740.12       False       5.727778   \n",
      "36      1    A           4  2/5/2010      39954.04       False       5.727778   \n",
      "48      1    A           5  2/5/2010      32229.38       False       5.727778   \n",
      "\n",
      "    fuel_price_usd_per_l  unemployment  \n",
      "0               0.679451         8.106  \n",
      "12              0.679451         8.106  \n",
      "24              0.679451         8.106  \n",
      "36              0.679451         8.106  \n",
      "48              0.679451         8.106  \n",
      "498      9/10/2010\n",
      "691     11/25/2011\n",
      "2315     2/12/2010\n",
      "6735      9/7/2012\n",
      "6810    12/31/2010\n",
      "6815     2/10/2012\n",
      "6820      9/9/2011\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=['store','type'])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=['store','department'])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales['is_holiday']==True].drop_duplicates(subset=['date'])\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Counting categorical variables</h3>\n",
    "Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you'll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n",
    "\n",
    "<li>Count the number of stores of each store type in store_types.</li>\n",
    "<li>Count the proportion of stores of each store type in store_types.</li>\n",
    "<li>Count the number of stores of each department in store_depts, sorting the counts in descending order.</li>\n",
    "<li>Count the proportion of stores of each department in store_depts, sorting the proportions in descending order.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    11\n",
      "B     1\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "A    0.916667\n",
      "B    0.083333\n",
      "Name: proportion, dtype: float64\n",
      "department\n",
      "1     12\n",
      "55    12\n",
      "72    12\n",
      "71    12\n",
      "67    12\n",
      "      ..\n",
      "37    10\n",
      "48     8\n",
      "50     6\n",
      "39     4\n",
      "43     2\n",
      "Name: count, Length: 80, dtype: int64\n",
      "department\n",
      "1     0.012917\n",
      "55    0.012917\n",
      "72    0.012917\n",
      "71    0.012917\n",
      "67    0.012917\n",
      "        ...   \n",
      "37    0.010764\n",
      "48    0.008611\n",
      "50    0.006459\n",
      "39    0.004306\n",
      "43    0.002153\n",
      "Name: proportion, Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stores of each type\n",
    "store_counts = store_types['type'].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types['type'].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of stores for each department and sort\n",
    "dept_counts_sorted = store_depts['department'].value_counts()\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of stores in each department and sort\n",
    "dept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3 style=\"color:orange;\">Grouped Summary Statistics</h3>\n",
    " What percent of sales occurred at each store type?\n",
    "\n",
    " While .groupby() is useful, you can calculate grouped summary statistics without it.\n",
    "\n",
    "Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each type.\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "<li>Calculate the total weekly_sales over the whole dataset.</li>\n",
    "<li>Subset for type \"A\" stores, and calculate their total weekly sales.</li>\n",
    "<li>Do the same for type \"B\" and type \"C\" stores.</li>\n",
    "<li>Combine the A/B/C results into a list, and divide by sales_all to get the proportion of sales by type.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9097747 0.0902253 0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"]==\"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"]==\"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type =[sales_A, sales_B,sales_C] / sales_all\n",
    "print(sales_propn_by_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3 style=\"color:orange;\">Calculations with .groupby()</h3>\n",
    " The .groupby() method makes life much easier. In this exercise, you'll perform the same calculations as last time, except you'll use the .groupby() method. You'll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it's a holiday week or not.\n",
    "\n",
    "sales is available and pandas is loaded as pd.\n",
    "\n",
    "<li>Group sales by \"type\", take the sum of \"weekly_sales\", and store as sales_by_type.</li>\n",
    "<li>Calculate the proportion of sales at each store type by dividing by the sum of sales_by_type. Assign to sales_propn_by_type.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    2.337163e+08\n",
      "B    2.317840e+07\n",
      "Name: weekly_sales, dtype: float64\n",
      "256894718.89999998\n",
      "type\n",
      "A    0.909775\n",
      "B    0.090225\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "print(sales_by_type)\n",
    "print(sum(sales_by_type))\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type / sum(sales_by_type)\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type  is_holiday\n",
      "A     False         2.336927e+08\n",
      "      True          2.360181e+04\n",
      "B     False         2.317678e+07\n",
      "      True          1.621410e+03\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday =  sales.groupby([\"type\",\"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Multiple grouped summaries</h3>\n",
    "\n",
    "Earlier in this chapter, you saw that the .agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median.\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "<li>Import numpy with the alias np.</li>\n",
    "<li>Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions!</li>\n",
    "<li>Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         min        max          mean    median\n",
      "type                                           \n",
      "A    -1098.0  293966.05  23674.667242  11943.92\n",
      "B     -798.0  232558.51  25696.678370  13336.08\n",
      "     unemployment                         fuel_price_usd_per_l            \\\n",
      "              min    max      mean median                  min       max   \n",
      "type                                                                       \n",
      "A           3.879  8.992  7.972611  8.067             0.664129  1.107410   \n",
      "B           7.170  9.765  9.279323  9.199             0.760023  1.107674   \n",
      "\n",
      "                          \n",
      "          mean    median  \n",
      "type                      \n",
      "A     0.744619  0.735455  \n",
      "B     0.805858  0.803348  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:5: FutureWarning: The provided callable <function min at 0x00000229FFC4B240> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:5: FutureWarning: The provided callable <function max at 0x00000229FFC4B100> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:5: FutureWarning: The provided callable <function mean at 0x00000229FFC4BB00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:5: FutureWarning: The provided callable <function median at 0x00000229FFD83100> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  sales_stats = sales.groupby(\"type\")['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:11: FutureWarning: The provided callable <function min at 0x00000229FFC4B240> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment','fuel_price_usd_per_l']].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:11: FutureWarning: The provided callable <function max at 0x00000229FFC4B100> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment','fuel_price_usd_per_l']].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:11: FutureWarning: The provided callable <function mean at 0x00000229FFC4BB00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment','fuel_price_usd_per_l']].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:11: FutureWarning: The provided callable <function median at 0x00000229FFD83100> is currently using SeriesGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment','fuel_price_usd_per_l']].agg([np.min,np.max,np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\361288484.py:11: FutureWarning: The provided callable <function min at 0x00000229FFC4B240> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  unemp_fuel_stats = sales.groupby('type')[['unemployment','fuel_price_usd_per_l']].agg([np.min,np.max,np.mean,np.median])\n"
     ]
    }
   ],
   "source": [
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby('type')[['unemployment','fuel_price_usd_per_l']].agg([np.min,np.max,np.mean,np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome aggregating! Notice that the minimum weekly_sales is negative because some stores had more returns than sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Pivoting on one variable</h3>\n",
    "Perfect pivoting! Pivot tables are another way to do the same thing as a group-by-then-summarize.\n",
    "\n",
    "Pivot tables are the standard way of aggregating data in spreadsheets.\n",
    "\n",
    "In pandas, pivot tables are essentially another way of performing grouped calculations. That is, the .pivot_table() method is an alternative to .groupby().\n",
    "\n",
    "In this exercise, you'll perform calculations using .pivot_table() to replicate the calculations you performed in the last lesson using .groupby().\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "<li>Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type.</li>\n",
    "<li>Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type.</li>\n",
    "<li>Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weekly_sales\n",
      "type              \n",
      "A     23674.667242\n",
      "B     25696.678370\n",
      "              mean       median\n",
      "      weekly_sales weekly_sales\n",
      "type                           \n",
      "A     23674.667242     11943.92\n",
      "B     25696.678370     13336.08\n",
      "is_holiday         False      True \n",
      "type                               \n",
      "A           23768.583523  590.04525\n",
      "B           25751.980533  810.70500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\3257438326.py:10: FutureWarning: The provided callable <function mean at 0x00000229FFC4BB00> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  mean_med_sales_by_type = sales.pivot_table(values='weekly_sales',index='type',aggfunc=[np.mean,np.median])\n",
      "C:\\Users\\RBTG\\AppData\\Local\\Temp\\ipykernel_21840\\3257438326.py:10: FutureWarning: The provided callable <function median at 0x00000229FFD83100> is currently using DataFrameGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  mean_med_sales_by_type = sales.pivot_table(values='weekly_sales',index='type',aggfunc=[np.mean,np.median])\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales',index='type')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values='weekly_sales',index='type',aggfunc=[np.mean,np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n",
    "\n",
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales',index='type',columns='is_holiday')\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:orange;\">Fill in missing values and sum values with pivot tables</h3>\n",
    "The .pivot_table() method has several useful arguments, including fill_value and margins.\n",
    "\n",
    "fill_value replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course (Dealing with Missing Data in Python), but the simplest thing to do is to substitute a dummy value.\n",
    "\n",
    "margins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.\n",
    "\n",
    "In this exercise, you'll practice using these arguments to up your pivot table skills, which will help you crunch numbers more efficiently!\n",
    "\n",
    "<li>Print the mean weekly_sales by department and type, filling in any missing values with 0.</li>\n",
    "<li>Print the mean weekly_sales by department and type, filling in any missing values with 0 and summing all rows and columns.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department             1              2             3             4  \\\n",
      "type                                                                  \n",
      "A           30961.725379   67600.158788  17160.002955  44285.399091   \n",
      "B           44050.626667  112958.526667  30580.655000  51219.654167   \n",
      "All         32052.467153   71380.022778  18278.390625  44863.253681   \n",
      "\n",
      "department             5             6             7             8  \\\n",
      "type                                                                 \n",
      "A           34821.011364   7136.292652  38454.336818  48583.475303   \n",
      "B           63236.875000  10717.297500  52909.653333  90733.753333   \n",
      "All         37189.000000   7434.709722  39658.946528  52095.998472   \n",
      "\n",
      "department             9            10  ...            91             92  \\\n",
      "type                                    ...                                \n",
      "A           30120.449924  30930.456364  ...  70423.165227  139722.204773   \n",
      "B           66679.301667  48595.126667  ...  13199.602500   50859.278333   \n",
      "All         33167.020903  32402.512222  ...  65654.535000  132316.960903   \n",
      "\n",
      "department            93            94             95            96  \\\n",
      "type                                                                  \n",
      "A           53413.633939  60081.155303  123933.787121  21367.042857   \n",
      "B            1466.274167    161.445833   77082.102500   9528.538333   \n",
      "All         49084.687292  55087.846181  120029.480069  20337.607681   \n",
      "\n",
      "department            97            98          99           All  \n",
      "type                                                              \n",
      "A           28471.266970  12875.423182  379.123659  23674.667242  \n",
      "B            5828.873333    217.428333    0.000000  25696.678370  \n",
      "All         26584.400833  11820.590278  379.123659  23843.950149  \n",
      "\n",
      "[3 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values='weekly_sales',index='type',columns='department',margins=True,fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                   A              B           All\n",
      "department                                           \n",
      "1           30961.725379   44050.626667  32052.467153\n",
      "2           67600.158788  112958.526667  71380.022778\n",
      "3           17160.002955   30580.655000  18278.390625\n",
      "4           44285.399091   51219.654167  44863.253681\n",
      "5           34821.011364   63236.875000  37189.000000\n",
      "...                  ...            ...           ...\n",
      "96          21367.042857    9528.538333  20337.607681\n",
      "97          28471.266970    5828.873333  26584.400833\n",
      "98          12875.423182     217.428333  11820.590278\n",
      "99            379.123659       0.000000    379.123659\n",
      "All         23674.667242   25696.678370  23843.950149\n",
      "\n",
      "[81 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0,margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:orange;\">Slicing and Indexing DataFrames</h3>\n",
    "<h4 style=\"color:orange;\">Setting and removing indexes</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances).\n",
    "\n",
    "In this chapter, you'll be exploring temperatures, a DataFrame of average temperatures in cities around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data for temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>avg_temp_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Abidjan</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>27.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>Abidjan</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>27.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>Abidjan</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>29.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>Abidjan</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>28.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>Abidjan</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>27.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16495</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>Xian</td>\n",
       "      <td>China</td>\n",
       "      <td>18.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16496</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>Xian</td>\n",
       "      <td>China</td>\n",
       "      <td>23.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16497</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>Xian</td>\n",
       "      <td>China</td>\n",
       "      <td>25.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16498</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>Xian</td>\n",
       "      <td>China</td>\n",
       "      <td>24.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16499</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>Xian</td>\n",
       "      <td>China</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date     city        country  avg_temp_c\n",
       "0      2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
       "1      2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
       "2      2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
       "3      2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
       "4      2000-05-01  Abidjan  Côte D'Ivoire      27.547\n",
       "...           ...      ...            ...         ...\n",
       "16495  2013-05-01     Xian          China      18.979\n",
       "16496  2013-06-01     Xian          China      23.522\n",
       "16497  2013-07-01     Xian          China      25.251\n",
       "16498  2013-08-01     Xian          China      24.528\n",
       "16499  2013-09-01     Xian          China         NaN\n",
       "\n",
       "[16500 rows x 4 columns]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatures = pd.read_csv('C:/Users/RBTG/OneDrive/Desktop/Data science/data/temperatures.csv', index_col = 0)\n",
    "temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the index of temperatures to \"city\", assigning to temperatures_ind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>avg_temp_c</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abidjan</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>27.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abidjan</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>27.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abidjan</th>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>29.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abidjan</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>28.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abidjan</th>\n",
       "      <td>2000-05-01</td>\n",
       "      <td>Côte D'Ivoire</td>\n",
       "      <td>27.547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date        country  avg_temp_c\n",
       "city                                          \n",
       "Abidjan  2000-01-01  Côte D'Ivoire      27.293\n",
       "Abidjan  2000-02-01  Côte D'Ivoire      27.685\n",
       "Abidjan  2000-03-01  Côte D'Ivoire      29.061\n",
       "Abidjan  2000-04-01  Côte D'Ivoire      28.162\n",
       "Abidjan  2000-05-01  Côte D'Ivoire      27.547"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the index of temperatures to city\n",
    "temperatures_ind =temperatures.set_index('city')\n",
    "temperatures_ind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the Index of Temperature_ind, keeping its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          city        date        country  avg_temp_c\n",
      "0      Abidjan  2000-01-01  Côte D'Ivoire      27.293\n",
      "1      Abidjan  2000-02-01  Côte D'Ivoire      27.685\n",
      "2      Abidjan  2000-03-01  Côte D'Ivoire      29.061\n",
      "3      Abidjan  2000-04-01  Côte D'Ivoire      28.162\n",
      "4      Abidjan  2000-05-01  Côte D'Ivoire      27.547\n",
      "...        ...         ...            ...         ...\n",
      "16495     Xian  2013-05-01          China      18.979\n",
      "16496     Xian  2013-06-01          China      23.522\n",
      "16497     Xian  2013-07-01          China      25.251\n",
      "16498     Xian  2013-08-01          China      24.528\n",
      "16499     Xian  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_ind.reset_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index of temperatures_ind, dropping its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date        country  avg_temp_c\n",
      "0      2000-01-01  Côte D'Ivoire      27.293\n",
      "1      2000-02-01  Côte D'Ivoire      27.685\n",
      "2      2000-03-01  Côte D'Ivoire      29.061\n",
      "3      2000-04-01  Côte D'Ivoire      28.162\n",
      "4      2000-05-01  Côte D'Ivoire      27.547\n",
      "...           ...            ...         ...\n",
      "16495  2013-05-01          China      18.979\n",
      "16496  2013-06-01          China      23.522\n",
      "16497  2013-07-01          China      25.251\n",
      "16498  2013-08-01          China      24.528\n",
      "16499  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_ind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Subsetting with .loc [ ]</h4>\n",
    "\n",
    "The killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.\n",
    "\n",
    "The code for subsetting using .loc[] can be easier to read than standard square bracket subsetting, which can make your code less burdensome to maintain.\n",
    "\n",
    "pandas is loaded as pd. temperatures and temperatures_ind are available; the latter is indexed by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list called cities that contains \"Moscow\" and \"Saint Petersburg\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\",\"Saint Petersburg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [ ] subsetting to filter temperatures for rows where the city column takes a value in the cities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date              city country  avg_temp_c\n",
      "10725  2000-01-01            Moscow  Russia      -7.313\n",
      "10726  2000-02-01            Moscow  Russia      -3.551\n",
      "10727  2000-03-01            Moscow  Russia      -1.661\n",
      "10728  2000-04-01            Moscow  Russia      10.096\n",
      "10729  2000-05-01            Moscow  Russia      10.357\n",
      "...           ...               ...     ...         ...\n",
      "13360  2013-05-01  Saint Petersburg  Russia      12.355\n",
      "13361  2013-06-01  Saint Petersburg  Russia      17.185\n",
      "13362  2013-07-01  Saint Petersburg  Russia      17.234\n",
      "13363  2013-08-01  Saint Petersburg  Russia      17.153\n",
      "13364  2013-09-01  Saint Petersburg  Russia         NaN\n",
      "\n",
      "[330 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures['city'].isin(cities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .loc [ ] subsetting to filter temperatures_ind for rows where the city is in the cities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date country  avg_temp_c\n",
      "city                                            \n",
      "Moscow            2000-01-01  Russia      -7.313\n",
      "Moscow            2000-02-01  Russia      -3.551\n",
      "Moscow            2000-03-01  Russia      -1.661\n",
      "Moscow            2000-04-01  Russia      10.096\n",
      "Moscow            2000-05-01  Russia      10.357\n",
      "...                      ...     ...         ...\n",
      "Saint Petersburg  2013-05-01  Russia      12.355\n",
      "Saint Petersburg  2013-06-01  Russia      17.185\n",
      "Saint Petersburg  2013-07-01  Russia      17.234\n",
      "Saint Petersburg  2013-08-01  Russia      17.153\n",
      "Saint Petersburg  2013-09-01  Russia         NaN\n",
      "\n",
      "[330 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_ind.loc[[\"Moscow\",\"Saint Petersburg\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Setting multi-level indexes</h4>\n",
    "\n",
    "Indexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). There is a trade-off to using these.\n",
    "\n",
    "The benefit is that multi-level indexes make it more natural to reason about nested categorical variables. For example, in a clinical trial, you might have control and treatment groups. Then each test subject belongs to one or another group, and we can say that a test subject is nested inside the treatment group. Similarly, in the temperature dataset, the city is located in the country, so we can say a city is nested inside the country.\n",
    "\n",
    "The main downside is that the code for manipulating indexes is different from the code for manipulating columns, so you have to learn two syntaxes and keep track of how your data is represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the index of temperatures to the \"country\" and \"city\" columns, and assign this to temperatures_ind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             date  avg_temp_c\n",
      "country       city                           \n",
      "Côte D'Ivoire Abidjan  2000-01-01      27.293\n",
      "              Abidjan  2000-02-01      27.685\n",
      "              Abidjan  2000-03-01      29.061\n",
      "              Abidjan  2000-04-01      28.162\n",
      "              Abidjan  2000-05-01      27.547\n",
      "...                           ...         ...\n",
      "China         Xian     2013-05-01      18.979\n",
      "              Xian     2013-06-01      23.522\n",
      "              Xian     2013-07-01      25.251\n",
      "              Xian     2013-08-01      24.528\n",
      "              Xian     2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures.set_index(['country','city']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify two country/city pairs to keep: \"Brazil\"/\"Rio De Janeiro\" and \"Pakistan\"/\"Lahore\", assigning to rows_to_keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_keep = [('Brazil', 'Rio De Janeiro'),('Pakistan', 'Lahore')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and subset temperatures_ind for rows_to_keep using .loc [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               date  avg_temp_c\n",
      "country  city                                  \n",
      "Brazil   Rio De Janeiro  2000-01-01      25.974\n",
      "         Rio De Janeiro  2000-02-01      26.699\n",
      "         Rio De Janeiro  2000-03-01      26.270\n",
      "         Rio De Janeiro  2000-04-01      25.750\n",
      "         Rio De Janeiro  2000-05-01      24.356\n",
      "...                             ...         ...\n",
      "Pakistan Lahore          2013-05-01      33.457\n",
      "         Lahore          2013-06-01      34.456\n",
      "         Lahore          2013-07-01      33.279\n",
      "         Lahore          2013-08-01      31.511\n",
      "         Lahore          2013-09-01         NaN\n",
      "\n",
      "[330 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures.set_index(['country','city']).loc[rows_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Sorting by index values</h4>\n",
    "\n",
    "Previously, you changed the order of the rows in a DataFrame by calling .sort_values ( ). It's also useful to be able to sort by elements in the index. For this, you need to use .sort_index ( ).\n",
    "\n",
    "pandas is loaded as pd. temperatures_ind has a multi-level index of country and city, and is available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort temperatures_ind by the index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          date  avg_temp_c\n",
      "country     city                          \n",
      "Afghanistan Kabul   2000-01-01       3.326\n",
      "            Kabul   2000-02-01       3.454\n",
      "            Kabul   2000-03-01       9.612\n",
      "            Kabul   2000-04-01      17.925\n",
      "            Kabul   2000-05-01      24.658\n",
      "...                        ...         ...\n",
      "Zimbabwe    Harare  2013-05-01      18.298\n",
      "            Harare  2013-06-01      17.020\n",
      "            Harare  2013-07-01      16.299\n",
      "            Harare  2013-08-01      19.232\n",
      "            Harare  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "temperatures_ind=temperatures.set_index(['country','city'])\n",
    "print(temperatures_ind.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort temperatures_ind by index values at the city level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             date  avg_temp_c\n",
      "country       city                           \n",
      "Côte D'Ivoire Abidjan  2000-01-01      27.293\n",
      "              Abidjan  2000-02-01      27.685\n",
      "              Abidjan  2000-03-01      29.061\n",
      "              Abidjan  2000-04-01      28.162\n",
      "              Abidjan  2000-05-01      27.547\n",
      "...                           ...         ...\n",
      "China         Xian     2013-05-01      18.979\n",
      "              Xian     2013-06-01      23.522\n",
      "              Xian     2013-07-01      25.251\n",
      "              Xian     2013-08-01      24.528\n",
      "              Xian     2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level='city'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort temperatures_ind by country then descending city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          date  avg_temp_c\n",
      "country     city                          \n",
      "Afghanistan Kabul   2000-01-01       3.326\n",
      "            Kabul   2000-02-01       3.454\n",
      "            Kabul   2000-03-01       9.612\n",
      "            Kabul   2000-04-01      17.925\n",
      "            Kabul   2000-05-01      24.658\n",
      "...                        ...         ...\n",
      "Zimbabwe    Harare  2013-05-01      18.298\n",
      "            Harare  2013-06-01      17.020\n",
      "            Harare  2013-07-01      16.299\n",
      "            Harare  2013-08-01      19.232\n",
      "            Harare  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=['country','city'],ascending=[True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Slicing index values</h4>\n",
    "\n",
    "Slicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we'll start with the first case. This involves slicing inside the .loc[] method.\n",
    "\n",
    "<li>You can only slice an index if the index is sorted (using .sort_index ( )).</li>\n",
    "<li>To slice at the outer level, first and last can be strings.</li>\n",
    "<li>To slice at inner levels, first and last should be tuples.</li>\n",
    "<li>If you pass a single slice to .loc [ ], it will slice the rows.</li>\n",
    "pandas is loaded as pd. temperatures_ind has country and city in the index, and is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the index of temperatures_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          date  avg_temp_c\n",
      "country     city                          \n",
      "Afghanistan Kabul   2000-01-01       3.326\n",
      "            Kabul   2000-02-01       3.454\n",
      "            Kabul   2000-03-01       9.612\n",
      "            Kabul   2000-04-01      17.925\n",
      "            Kabul   2000-05-01      24.658\n",
      "...                        ...         ...\n",
      "Zimbabwe    Harare  2013-05-01      18.298\n",
      "            Harare  2013-06-01      17.020\n",
      "            Harare  2013-07-01      16.299\n",
      "            Harare  2013-08-01      19.232\n",
      "            Harare  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "print(temperatures_srt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Subset rows from Pakistan to Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 date  avg_temp_c\n",
      "country  city                                    \n",
      "Pakistan Faisalabad        2000-01-01      12.792\n",
      "         Faisalabad        2000-02-01      14.339\n",
      "         Faisalabad        2000-03-01      20.309\n",
      "         Faisalabad        2000-04-01      29.072\n",
      "         Faisalabad        2000-05-01      34.845\n",
      "...                               ...         ...\n",
      "Russia   Saint Petersburg  2013-05-01      12.355\n",
      "         Saint Petersburg  2013-06-01      17.185\n",
      "         Saint Petersburg  2013-07-01      17.234\n",
      "         Saint Petersburg  2013-08-01      17.153\n",
      "         Saint Petersburg  2013-09-01         NaN\n",
      "\n",
      "[1155 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_srt.loc['Pakistan':'Russia',:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to subset rows from Lahore to Moscow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  avg_temp_c\n",
      "country  city                          \n",
      "Pakistan Lahore  2000-01-01      12.792\n",
      "         Lahore  2000-02-01      14.339\n",
      "         Lahore  2000-03-01      20.309\n",
      "         Lahore  2000-04-01      29.072\n",
      "         Lahore  2000-05-01      34.845\n",
      "...                     ...         ...\n",
      "Russia   Moscow  2013-05-01      16.152\n",
      "         Moscow  2013-06-01      18.718\n",
      "         Moscow  2013-07-01      18.136\n",
      "         Moscow  2013-08-01      17.485\n",
      "         Moscow  2013-09-01         NaN\n",
      "\n",
      "[660 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_srt.loc[('Pakistan','Lahore'):('Russia','Moscow'),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Slicing in both directions</h4>\n",
    "\n",
    "You've seen slicing DataFrames by rows and by columns, but since DataFrames are two-dimensional objects, it is often natural to slice both dimensions at once. That is, by passing two arguments to .loc [ ], you can subset by rows and columns in one go.\n",
    "\n",
    "pandas is loaded as pd. temperatures_srt is indexed by country and city, has a sorted index, and is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .loc [ ] slicing to subset rows from India, Hyderabad to Iraq, Baghdad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         date  avg_temp_c\n",
      "country city                             \n",
      "India   Hyderabad  2000-01-01      23.779\n",
      "        Hyderabad  2000-02-01      25.826\n",
      "        Hyderabad  2000-03-01      28.821\n",
      "        Hyderabad  2000-04-01      32.698\n",
      "        Hyderabad  2000-05-01      32.438\n",
      "...                       ...         ...\n",
      "Iraq    Baghdad    2013-05-01      28.673\n",
      "        Baghdad    2013-06-01      33.803\n",
      "        Baghdad    2013-07-01      36.392\n",
      "        Baghdad    2013-08-01      35.463\n",
      "        Baghdad    2013-09-01         NaN\n",
      "\n",
      "[2145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[('India','Hyderabad'):('Iraq','Baghdad')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .loc[] slicing to subset columns from date to avg_temp_c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          date  avg_temp_c\n",
      "country     city                          \n",
      "Afghanistan Kabul   2000-01-01       3.326\n",
      "            Kabul   2000-02-01       3.454\n",
      "            Kabul   2000-03-01       9.612\n",
      "            Kabul   2000-04-01      17.925\n",
      "            Kabul   2000-05-01      24.658\n",
      "...                        ...         ...\n",
      "Zimbabwe    Harare  2013-05-01      18.298\n",
      "            Harare  2013-06-01      17.020\n",
      "            Harare  2013-07-01      16.299\n",
      "            Harare  2013-08-01      19.232\n",
      "            Harare  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:,'date':'avg_temp_c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice in both directions at once from Hyderabad to Baghdad, and date to avg_temp_c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         date  avg_temp_c\n",
      "country city                             \n",
      "India   Hyderabad  2000-01-01      23.779\n",
      "        Hyderabad  2000-02-01      25.826\n",
      "        Hyderabad  2000-03-01      28.821\n",
      "        Hyderabad  2000-04-01      32.698\n",
      "        Hyderabad  2000-05-01      32.438\n",
      "...                       ...         ...\n",
      "Iraq    Baghdad    2013-05-01      28.673\n",
      "        Baghdad    2013-06-01      33.803\n",
      "        Baghdad    2013-07-01      36.392\n",
      "        Baghdad    2013-08-01      35.463\n",
      "        Baghdad    2013-09-01         NaN\n",
      "\n",
      "[2145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[('India','Hyderabad'):('Iraq','Baghdad'),'date':'avg_temp_c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Slicing time series</h4>\n",
    "\n",
    "Slicing is particularly useful for time series since it's a common thing to want to filter for data within a date range. Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, \"yyyy-mm-dd\" for year-month-day, \"yyyy-mm\" for year-month, and \"yyyy\" for year.\n",
    "\n",
    "Recall from Chapter 1 that you can combine multiple Boolean conditions using logical operators, such as &. To do so in one line of code, you'll need to add parentheses () around each condition.\n",
    "\n",
    "pandas is loaded as pd and temperatures, with no index, is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Boolean conditions, not .isin() or .loc[], and the full date \"yyyy-mm-dd\", to subset temperatures for rows where the date column is in 2010 and 2011 and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date     city        country  avg_temp_c\n",
      "129    2010-10-01  Abidjan  Côte D'Ivoire      26.397\n",
      "130    2010-11-01  Abidjan  Côte D'Ivoire      27.446\n",
      "131    2010-12-01  Abidjan  Côte D'Ivoire      27.666\n",
      "132    2011-01-01  Abidjan  Côte D'Ivoire      27.360\n",
      "133    2011-02-01  Abidjan  Côte D'Ivoire      28.295\n",
      "...           ...      ...            ...         ...\n",
      "16474  2011-08-01     Xian          China      23.069\n",
      "16475  2011-09-01     Xian          China      16.775\n",
      "16476  2011-10-01     Xian          China      12.587\n",
      "16477  2011-11-01     Xian          China       7.543\n",
      "16478  2011-12-01     Xian          China      -0.490\n",
      "\n",
      "[1500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures['date'] >= '2010-1-1') & (temperatures['date'] <= '2011-12-01')]\n",
    "print(temperatures_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set date as the index and sort the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city        country  avg_temp_c\n",
      "date                                            \n",
      "2000-01-01    Abidjan  Côte D'Ivoire      27.293\n",
      "2000-01-01     Lahore       Pakistan      12.792\n",
      "2000-01-01   Tangshan          China      -5.406\n",
      "2000-01-01      Gizeh          Egypt      12.669\n",
      "2000-01-01    Lakhnau          India      15.152\n",
      "...               ...            ...         ...\n",
      "2013-09-01    Nanjing          China         NaN\n",
      "2013-09-01  New Delhi          India         NaN\n",
      "2013-09-01   New York  United States      17.408\n",
      "2013-09-01     Peking          China         NaN\n",
      "2013-09-01       Xian          China         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index('date').sort_index()\n",
    "print(temperatures_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .loc [ ] to subset temperatures_ind for rows in 2010 and 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  city    country  avg_temp_c\n",
      "date                                         \n",
      "2010-01-01  Faisalabad   Pakistan      11.810\n",
      "2010-01-01   Melbourne  Australia      20.016\n",
      "2010-01-01   Chongqing      China       7.921\n",
      "2010-01-01   São Paulo     Brazil      23.738\n",
      "2010-01-01   Guangzhou      China      14.136\n",
      "...                ...        ...         ...\n",
      "2010-12-01     Jakarta  Indonesia      26.602\n",
      "2010-12-01       Gizeh      Egypt      16.530\n",
      "2010-12-01      Nagpur      India      19.120\n",
      "2010-12-01      Sydney  Australia      19.559\n",
      "2010-12-01    Salvador     Brazil      26.265\n",
      "\n",
      "[1200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_ind.loc['2010':'2011'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .loc [ ] to subset temperatures_ind for rows from August 2010 to February 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     city        country  avg_temp_c\n",
      "date                                                \n",
      "2010-08-01       Calcutta          India      30.226\n",
      "2010-08-01           Pune          India      24.941\n",
      "2010-08-01          Izmir         Turkey      28.352\n",
      "2010-08-01        Tianjin          China      25.543\n",
      "2010-08-01         Manila    Philippines      27.101\n",
      "...                   ...            ...         ...\n",
      "2011-01-01  Dar Es Salaam       Tanzania      28.541\n",
      "2011-01-01        Nairobi          Kenya      17.768\n",
      "2011-01-01    Addis Abeba       Ethiopia      17.708\n",
      "2011-01-01        Nanjing          China       0.144\n",
      "2011-01-01       New York  United States      -4.463\n",
      "\n",
      "[600 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(temperatures_ind.loc['2010-08':'2011-02'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange;\">Subsetting by row/column number</h4>\n",
    "\n",
    "The most common ways to subset rows are the ways we've previously discussed: using a Boolean condition or by index labels. However, it is also occasionally useful to pass row numbers.\n",
    "\n",
    "This is done using .iloc [ ], and like .loc [ ], it can take two arguments to let you subset by rows and columns.\n",
    "\n",
    "pandas is loaded as pd. temperatures (without an index) is available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 23rd row, 2nd column (index positions 22 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abidjan\n"
     ]
    }
   ],
   "source": [
    "print(temperatures.iloc[23,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first 5 rows (index positions 0 to 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date     city        country  avg_temp_c\n",
      "0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
      "1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
      "2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
      "3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
      "4  2000-05-01  Abidjan  Côte D'Ivoire      27.547\n"
     ]
    }
   ],
   "source": [
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use slicing to get columns 3 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             country  avg_temp_c\n",
      "0      Côte D'Ivoire      27.293\n",
      "1      Côte D'Ivoire      27.685\n",
      "2      Côte D'Ivoire      29.061\n",
      "3      Côte D'Ivoire      28.162\n",
      "4      Côte D'Ivoire      27.547\n",
      "...              ...         ...\n",
      "16495          China      18.979\n",
      "16496          China      23.522\n",
      "16497          China      25.251\n",
      "16498          China      24.528\n",
      "16499          China         NaN\n",
      "\n",
      "[16500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:,2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use slicing in both directions at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         country  avg_temp_c\n",
      "0  Côte D'Ivoire      27.293\n",
      "1  Côte D'Ivoire      27.685\n",
      "2  Côte D'Ivoire      29.061\n",
      "3  Côte D'Ivoire      28.162\n",
      "4  Côte D'Ivoire      27.547\n"
     ]
    }
   ],
   "source": [
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[:5,2:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
